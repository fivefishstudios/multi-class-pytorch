{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1OeaW0FHC8G"
      },
      "source": [
        "## 8. Putting things together by building a multi-class PyTorch model\n",
        "\n",
        "We've covered a fair bit.\n",
        "\n",
        "But now let's put it all together using a multi-class classification problem.\n",
        "\n",
        "Recall a **binary classification** problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a **multi-class classification** problem deals with classifying something from a list of *more than* two options (e.g. classifying a photo as a cat a dog or a chicken).\n",
        "\n",
        "![binary vs multi-class classification image with the example of dog vs cat for binary classification and dog vs cat vs chicken for multi-class classification](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-binary-vs-multi-class-classification.png)\n",
        "*Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular [ImageNet-1k dataset](https://www.image-net.org/) is used as a computer vision benchmark and has 1000 classes.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Ephtx6f1jB"
      },
      "source": [
        "### 8.1 Creating mutli-class classification data\n",
        "\n",
        "To begin a multi-class classification problem, let's create some multi-class data.\n",
        "\n",
        "To do so, we can leverage Scikit-Learn's [`make_blobs()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) method.\n",
        "\n",
        "This method will create however many classes (using the `centers` parameter) we want.\n",
        "\n",
        "Specifically, let's do the following:\n",
        "\n",
        "1. Create some multi-class data with `make_blobs()`.\n",
        "2. Turn the data into tensors (the default of `make_blobs()` is to use NumPy arrays).\n",
        "3. Split the data into training and test sets using `train_test_split()`.\n",
        "4. Visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "x3_UwcwHHC8G",
        "outputId": "cc15c9be-70dd-44c8-bec3-40027f18cef4"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set the hyperparameters for data creation\n",
        "NUM_SAMPLES = 4000\n",
        "NUM_CLASSES = 10\n",
        "CLUSTER_STD = 1.0\n",
        "NUM_FEATURES = 2\n",
        "NUM_NODES = 4\n",
        "RANDOM_SEED = 999\n",
        "LEARNING_RATE = 0.05\n",
        "NUM_EPOCHS = 150000\n",
        "\n",
        "# =========\n",
        "# 30 -> 19%\n",
        "# 50 -> 30%\n",
        "#   100 -> 71%\n",
        "#   200 -> 68%\n",
        "#   300 -> 71%\n",
        "#   400 -> 68%\n",
        "#   1000 -> 72%\n",
        "#   10K -> 77%\n",
        "#   25K -> 77%\n",
        "#   50K -> 80%\n",
        "#  100K -> 80%\n",
        "#  150K -> 78% -> 87% -> 91%\n",
        "#  200K -> 9%\n",
        "\n",
        "# 1. Create multi-class data\n",
        "X_blob, y_blob = make_blobs(n_samples=NUM_SAMPLES,\n",
        "    n_features=NUM_FEATURES, # X features\n",
        "    centers=NUM_CLASSES, # y labels \n",
        "    cluster_std=CLUSTER_STD, # give the clusters a little shake up (try changing this to 1.0, the default)\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 2. Turn data into tensors\n",
        "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
        "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) # LongTensor\n",
        "print(X_blob[:5], y_blob[:5])\n",
        "\n",
        "# 3. Split into train and test sets\n",
        "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n",
        "    y_blob,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "# 4. Plot data\n",
        "%matplotlib inline \n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdS2eQkCmfeF",
        "outputId": "0de24b18-2b16-43ca-c8f6-3727a6e4141c"
      },
      "outputs": [],
      "source": [
        "# download helper functions\n",
        "import requests\n",
        "from pathlib import Path \n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n",
        "from helper_functions import plot_predictions, plot_decision_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCnUs0oHC8G"
      },
      "source": [
        "Nice! Looks like we've got some multi-class data ready to go.\n",
        "\n",
        "Let's build a model to separate the coloured blobs. \n",
        "\n",
        "> **Question:** Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dINSA0Chiof"
      },
      "source": [
        "### 8.2 Building a multi-class classification model in PyTorch\n",
        "\n",
        "We've created a few models in PyTorch so far.\n",
        "\n",
        "You might also be starting to get an idea of how flexible neural networks are.\n",
        "\n",
        "How about we build one similar to `model_3` but this still capable of handling multi-class data?\n",
        "\n",
        "To do so, let's create a subclass of `nn.Module` that takes in three hyperparameters:\n",
        "* `input_features` - the number of `X` features coming into the model.\n",
        "* `output_features` - the ideal numbers of output features we'd like (this will be equivalent to `NUM_CLASSES` or the number of classes in your multi-class classification problem).\n",
        "* `hidden_units` - the number of hidden neurons we'd like each hidden layer to use.\n",
        "\n",
        "Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).\n",
        "\n",
        "Then we'll create the model class using the hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g9OPjDfQk1AU",
        "outputId": "39bb640a-8020-4972-d539-19806cf8909b"
      },
      "outputs": [],
      "source": [
        "# Create device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGoZCzOrHC8H",
        "outputId": "8e7ce72f-457f-4620-e1ee-b79c32208314"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# Build model\n",
        "class BlobModel(nn.Module):\n",
        "    def __init__(self, input_features, output_features, hidden_units=8):\n",
        "        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n",
        "\n",
        "        Args:\n",
        "            input_features (int): Number of input features to the model.\n",
        "            out_features (int): Number of output features of the model\n",
        "              (how many classes there are).\n",
        "            hidden_units (int): Number of hidden units between layers, default 8.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear_layer_stack = nn.Sequential(\n",
        "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
        "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
        "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n",
        "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)     \n",
        "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
        "            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)            \n",
        "            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear_layer_stack(x)\n",
        "\n",
        "# Create an instance of BlobModel and send it to the target device\n",
        "model_4 = BlobModel(input_features=NUM_FEATURES, \n",
        "                    output_features=NUM_CLASSES, \n",
        "                    hidden_units = NUM_NODES).to(device)\n",
        "model_4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eOASXWAHC8H"
      },
      "source": [
        "Excellent! Our multi-class model is ready to go, let's create a loss function and optimizer for it.\n",
        "\n",
        "### 8.3 Creating a loss function and optimizer for a multi-class PyTorch model\n",
        "\n",
        "Since we're working on a multi-class classification problem, we'll use the `nn.CrossEntropyLoss()` method as our loss function.\n",
        "\n",
        "And we'll stick with using SGD with a learning rate of 0.1 for optimizing our `model_4` parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngHLo--HC8H"
      },
      "outputs": [],
      "source": [
        "# Create loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_4.parameters(), \n",
        "                            lr=LEARNING_RATE) # exercise: try changing the learning rate here and seeing what happens to the model's performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orcVVmLzo3gX"
      },
      "source": [
        "### 8.4 Getting prediction probabilities for a multi-class PyTorch model\n",
        "\n",
        "Alright, we've got a loss function and optimizer ready, and we're ready to train our model but before we do let's do a single forward pass with our model to see if it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--d6YmldpZh_",
        "outputId": "0dac1f8c-a3ef-4482-b3a6-f5da6735bd4f"
      },
      "outputs": [],
      "source": [
        "# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n",
        "model_4(X_blob_train.to(device))[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fk1K9VlpoPI"
      },
      "source": [
        "What's coming out here?\n",
        "\n",
        "It looks like we get one value per feature of each sample.\n",
        "\n",
        "Let's check the shape to confirm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W4jnvmWp0OH",
        "outputId": "1cf213d2-a3e7-4677-836d-1adf3f09e6be"
      },
      "outputs": [],
      "source": [
        "# How many elements in a single prediction sample?\n",
        "model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyQSNaSVqBBN"
      },
      "source": [
        "Wonderful, our model is predicting one value for each class that we have.\n",
        "\n",
        "Do you remember what the raw outputs of our model are called?\n",
        "\n",
        "Hint: it rhymes with \"frog splits\" (no animals were harmed in the creation of these materials).\n",
        "\n",
        "If you guessed *logits*, you'd be correct.\n",
        "\n",
        "So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?\n",
        "\n",
        "As in, how do we go from `logits -> prediction probabilities -> prediction labels` just like we did with the binary classification problem?\n",
        "\n",
        "That's where the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) comes into play.\n",
        "\n",
        "The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.\n",
        "\n",
        "If this doesn't make sense, let's see in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hU_Wxudrbiq",
        "outputId": "48f8fe09-681d-4285-aa67-fc9b4de5738d"
      },
      "outputs": [],
      "source": [
        "# Make prediction logits with model\n",
        "y_logits = model_4(X_blob_test.to(device))\n",
        "\n",
        "# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1) \n",
        "print(y_logits[:5])\n",
        "print(y_pred_probs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_pbSytSrzHF"
      },
      "source": [
        "Hmm, what's happened here?\n",
        "\n",
        "It may still look like the outputs of the softmax function are jumbled numbers (and they are, since our model hasn't been trained and is predicting using random patterns) but there's a very specific thing different about each sample.\n",
        "\n",
        "After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to).\n",
        "\n",
        "Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fC5No7IsSiB",
        "outputId": "2d35bf15-4e73-4cc4-8505-8c8c79943de5"
      },
      "outputs": [],
      "source": [
        "# Sum the first sample output of the softmax activation function \n",
        "torch.sum(y_pred_probs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhwu9ln1sbl7"
      },
      "source": [
        "These prediction probablities are essentially saying how much the model *thinks* the target `X` sample (the input) maps to each class.\n",
        "\n",
        "Since there's one value for each class in `y_pred_probs`, the index of the *highest* value is the class the model thinks the specific data sample *most* belongs to.\n",
        "\n",
        "We can check which index has the highest value using `torch.argmax()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X3Yf5gCsbME",
        "outputId": "160d7f1b-0a47-4c1d-8522-b67bfe320df4"
      },
      "outputs": [],
      "source": [
        "# Which class does the model think is *most* likely at the index 0 sample?\n",
        "print(y_pred_probs[0])\n",
        "print(torch.argmax(y_pred_probs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9veE071JtSZJ"
      },
      "source": [
        "You can see the output of `torch.argmax()` returns 3, so for the features (`X`) of the sample at index 0, the model is predicting that the most likely class value (`y`) is 3.\n",
        "\n",
        "Of course, right now this is just random guessing so it's got a 25% chance of being right (since there's four classes). But we can improve those chances by training the model.\n",
        "\n",
        "> **Note:** To summarize the above, a model's raw output is referred to as **logits**.\n",
        "> \n",
        "> For a multi-class classification problem, to turn the logits into **prediction probabilities**, you use the softmax activation function (`torch.softmax`).\n",
        ">\n",
        "> The index of the value with the highest **prediction probability** is the class number the model thinks is *most* likely given the input features for that sample (although this is a prediction, it doesn't mean it will be correct)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlqJ3_xTupU3"
      },
      "source": [
        "### 8.5 Creating a training and testing loop for a multi-class PyTorch model\n",
        "\n",
        "Alright, now we've got all of the preparation steps out of the way, let's write a training and testing loop to improve and evaluation our model.\n",
        "\n",
        "We've done many of these steps before so much of this will be practice.\n",
        "\n",
        "The only difference is that we'll be adjusting the steps to turn the model outputs (logits) to prediction probabilities (using the softmax activation function) and then to prediction labels (by taking the argmax of the output of the softmax activation function).\n",
        "\n",
        "Let's train the model for `epochs=100` and evaluate it every 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55s1Pis9HC8H",
        "outputId": "d4e0faee-aa19-4080-df23-9497c59e6469"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = NUM_EPOCHS\n",
        "\n",
        "# Put data to target device\n",
        "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
        "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    model_4.train()\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_logits = model_4(X_blob_train) # model outputs raw logits \n",
        "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
        "    # print(y_logits)\n",
        "    # 2. Calculate loss and accuracy\n",
        "    loss = loss_fn(y_logits, y_blob_train) \n",
        "    # acc = accuracy_fn(y_true=y_blob_train,\n",
        "    #                   y_pred=y_pred)\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    # Print out what's happening\n",
        "    if epoch % 100 == 0:\n",
        "      model_4.eval()\n",
        "      with torch.inference_mode():\n",
        "        # 1. Forward pass\n",
        "        test_logits = model_4(X_blob_test)\n",
        "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
        "        # 2. Calculate test loss and accuracy\n",
        "        test_loss = loss_fn(test_logits, y_blob_test)\n",
        "        # test_acc = accuracy_fn(y_true=y_blob_test,\n",
        "        #                        y_pred=test_pred)\n",
        "\n",
        "\n",
        "      # print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\") \n",
        "      print(f\"Epoch: {epoch} | Loss: {loss:.5f}, | Test Loss: {test_loss:.5f}, \") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_JNlpd4L6dL"
      },
      "source": [
        "### 8.6 Making and evaluating predictions with a PyTorch multi-class model\n",
        "\n",
        "It looks like our trained model is performaning pretty well.\n",
        "\n",
        "But to make sure of this, let's make some predictions and visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjCKKhsGHC8H",
        "outputId": "e8f16e70-e89e-43d1-83d4-6de6d3e7b99c"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "    y_logits = model_4(X_blob_test)\n",
        "\n",
        "# View the first 10 predictions\n",
        "y_logits[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpAdeJRMNHjG"
      },
      "source": [
        "Alright, looks like our model's predictions are still in logit form.\n",
        "\n",
        "Though to evaluate them, they'll have to be in the same form as our labels (`y_blob_test`) which are in integer form.\n",
        "\n",
        "Let's convert our model's prediction logits to prediction probabilities (using `torch.softmax()`) then to prediction labels (by taking the `argmax()` of each sample).\n",
        "\n",
        "> **Note:** It's possible to skip the `torch.softmax()` function and go straight from `predicted logits -> predicted labels` by calling `torch.argmax()` directly on the logits.\n",
        ">\n",
        "> For example, `y_preds = torch.argmax(y_logits, dim=1)`, this saves a computation step (no `torch.softmax()`) but results in no prediction probabilities being available to use. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faDQ4oLpHC8H",
        "outputId": "6021020b-d125-4214-a12c-e4a072fe2797"
      },
      "outputs": [],
      "source": [
        "# Turn predicted logits in prediction probabilities\n",
        "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
        "\n",
        "# Turn prediction probabilities into prediction labels\n",
        "y_preds = y_pred_probs.argmax(dim=1)\n",
        "\n",
        "# Compare first 10 model preds and test labels\n",
        "print(f\"Preds : {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\n",
        "# print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMA5SSixOSru"
      },
      "source": [
        "Nice! Our model predictions are now in the same form as our test labels.\n",
        "\n",
        "Let's visualize them with `plot_decision_boundary()`, remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (`plot_decision_boundary()` does this automatically for us)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "kLOzBFdRHC8I",
        "outputId": "898abc51-76d2-45f8-8d31-af91c097d111"
      },
      "outputs": [],
      "source": [
        "if NUM_FEATURES == 2:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Train\")\n",
        "    plot_decision_boundary(model_4, X_blob_train, y_blob_train)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Test\")\n",
        "    plot_decision_boundary(model_4, X_blob_test, y_blob_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0vFchUQx7mc"
      },
      "source": [
        "## 9. More classification evaluation metrics\n",
        "\n",
        "So far we've only covered a couple of ways of evaluating a classification model (accuracy, loss and visualizing predictions).\n",
        "\n",
        "These are some of the most common methods you'll come across and are a good starting point.\n",
        "\n",
        "However, you may want to evaluate you classification model using more metrics such as the following:\n",
        "\n",
        "| **Metric name/Evaluation method** | **Defintion** | **Code** |\n",
        "| --- | --- | --- |\n",
        "| Accuracy | Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. | [`torchmetrics.Accuracy()`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id3) or [`sklearn.metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) |\n",
        "| Precision | Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). | [`torchmetrics.Precision()`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id4) or [`sklearn.metrics.precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) |\n",
        "| Recall | Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. | [`torchmetrics.Recall()`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#id5) or [`sklearn.metrics.recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) |\n",
        "| F1-score | Combines precision and recall into one metric. 1 is best, 0 is worst. | [`torchmetrics.F1Score()`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#f1score) or [`sklearn.metrics.f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) |\n",
        "| [Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)  | Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). | [`torchmetrics.ConfusionMatrix`](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#confusionmatrix) or [`sklearn.metrics.plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions) |\n",
        "| Classification report | Collection of some of the main classification metrics such as precision, recall and f1-score. | [`sklearn.metrics.classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) |\n",
        "\n",
        "Scikit-Learn (a popular and world-class machine learning library) has many implementations of the above metrics and you're looking for a PyTorch-like version, check out [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/), especially the [TorchMetrics classification section](https://torchmetrics.readthedocs.io/en/latest/references/modules.html#classification). \n",
        "\n",
        "Let's try the `torchmetrics.Accuracy` metric out.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_HrLXfutFFX",
        "outputId": "f57e1476-4895-4934-a40c-973cf6dedac2"
      },
      "outputs": [],
      "source": [
        "# !pip -q install torchmetrics\n",
        "import sklearn \n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Setup metric and make sure it's on the target device\n",
        "torchmetrics_accuracy = Accuracy().to(device)\n",
        "\n",
        "# Calculate accuracy\n",
        "tm_acc = torchmetrics_accuracy(y_preds, y_blob_test)\n",
        "sk_acc = sklearn.metrics.accuracy_score(y_preds, y_blob_test)\n",
        "\n",
        "print(\"torch metrics acc:\", tm_acc)\n",
        "print(\"sklearn metrics acc:\", sk_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4d-S9_-HC8I"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "All of the exercises are focused on practicing the code in the sections above.\n",
        "\n",
        "You should be able to complete them by referencing each section or by following the resource(s) linked.\n",
        "\n",
        "All exercises should be completed using [device-agonistic code](https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code).\n",
        "\n",
        "Resources:\n",
        "* [Exercise template notebook for 02](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/02_pytorch_classification_exercises.ipynb)\n",
        "* [Example solutions notebook for 02](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/02_pytorch_classification_exercise_solutions.ipynb) (try the exercises *before* looking at this)\n",
        "\n",
        "1. Make a binary classification dataset with Scikit-Learn's [`make_moons()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function.\n",
        "    * For consistency, the dataset should have 1000 samples and a `random_state=42`.\n",
        "    * Turn the data into PyTorch tensors. Split the data into training and test sets using `train_test_split` with 80% training and 20% testing.\n",
        "2. Build a model by subclassing `nn.Module` that incorporates non-linear activation functions and is capable of fitting the data you created in 1.\n",
        "    * Feel free to use any combination of PyTorch layers (linear and non-linear) you want.\n",
        "3. Setup a binary classification compatible loss function and optimizer to use when training the model.\n",
        "4. Create a training and testing loop to fit the model you created in 2 to the data you created in 1.\n",
        "    * To measure model accuray, you can create your own accuracy function or use the accuracy function in [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/).\n",
        "    * Train the model for long enough for it to reach over 96% accuracy.\n",
        "    * The training loop should output progress every 10 epochs of the model's training and test set loss and accuracy.\n",
        "5. Make predictions with your trained model and plot them using the `plot_decision_boundary()` function created in this notebook.\n",
        "6. Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.\n",
        "    * Feel free to reference the [ML cheatsheet website](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh) for the formula.\n",
        "7. Create a multi-class dataset using the [spirals data creation function from CS231n](https://cs231n.github.io/neural-networks-case-study/) (see below for the code).\n",
        "    * Construct a model capable of fitting the data (you may need a combination of linear and non-linear layers).\n",
        "    * Build a loss function and optimizer capable of handling multi-class data (optional extension: use the Adam optimizer instead of SGD, you may have to experiment with different values of the learning rate to get it working).\n",
        "    * Make a training and testing loop for the multi-class data and train a model on it to reach over 95% testing accuracy (you can use any accuracy measuring function here that you like).\n",
        "    * Plot the decision boundaries on the spirals dataset from your model predictions, the `plot_decision_boundary()` function should work for this dataset too.\n",
        "\n",
        "```python\n",
        "# Code for creating a spiral dataset from CS231n\n",
        "import numpy as np\n",
        "N = 100 # number of points per class\n",
        "D = 2 # dimensionality\n",
        "K = 3 # number of classes\n",
        "X = np.zeros((N*K,D)) # data matrix (each row = single example)\n",
        "y = np.zeros(N*K, dtype='uint8') # class labels\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "  y[ix] = j\n",
        "# lets visualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Extra-curriculum\n",
        "\n",
        "* Write down 3 problems where you think machine classification could be useful (these can be anything, get creative as you like, for example, classifying credit card transactions as fraud or not fraud based on the purchase amount and purchase location features). \n",
        "* Research the concept of \"momentum\" in gradient-based optimizers (like SGD or Adam), what does it mean?\n",
        "* Spend 10-minutes reading the [Wikipedia page for different activation functions](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions), how many of these can you line up with [PyTorch's activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)?\n",
        "* Research when accuracy might be a poor metric to use (hint: read [\"Beyond Accuracy\" by by Will Koehrsen](https://willkoehrsen.github.io/statistics/learning/beyond-accuracy-precision-and-recall/) for ideas).\n",
        "* **Watch:** For an idea of what's happening within our neural networks and what they're doing to learn, watch [MIT's Introduction to Deep Learning video](https://youtu.be/7sB052Pz0sQ)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('MachineLearning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "417b8a5586138b52e6e330676d25af649515914f0752835194bc61a7bead1e7c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
